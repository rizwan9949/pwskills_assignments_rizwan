{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Simple Linear Regression\n",
        "\n",
        "# 1. What is Simple Linear Regression?\n",
        "# Simple Linear Regression models the relationship between a dependent variable (Y)\n",
        "# and an independent variable (X) using the equation: Y = mX + c.\n",
        "\n",
        "# 2. What are the key assumptions of Simple Linear Regression?\n",
        "# - Linearity: The relationship between X and Y is linear.\n",
        "# - Independence: Observations are independent.\n",
        "# - Homoscedasticity: Constant variance of residuals.\n",
        "# - Normality: Residuals should follow a normal distribution.\n",
        "# - No Multicollinearity: Only one independent variable.\n",
        "\n",
        "# 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "# The coefficient m represents the slope, indicating how much Y changes when X increases by 1 unit.\n",
        "\n",
        "# 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "# The intercept c is the value of Y when X = 0.\n",
        "\n",
        "# 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "# Formula: m = (Œ£(Xi - XÃÑ)(Yi - »≤)) / (Œ£(Xi - XÃÑ)¬≤)\n",
        "\n",
        "# 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "# It minimizes the sum of squared residuals to find the best-fit regression line.\n",
        "\n",
        "# 7. How is the coefficient of determination (R¬≤) interpreted?\n",
        "# R¬≤ measures how well the regression line fits the data. A value close to 1 indicates a good fit.\n",
        "\n",
        "# üìå Multiple Linear Regression\n",
        "\n",
        "# 8. What is Multiple Linear Regression?\n",
        "# A regression model that includes two or more independent variables to predict Y.\n",
        "\n",
        "# 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "# - Simple Linear Regression: 1 independent variable.\n",
        "# - Multiple Linear Regression: 2 or more independent variables.\n",
        "\n",
        "# 10. What are the key assumptions of Multiple Linear Regression?\n",
        "# - Linearity\n",
        "# - Independence\n",
        "# - Homoscedasticity\n",
        "# - Normality of residuals\n",
        "# - No Multicollinearity\n",
        "\n",
        "# 11. What is heteroscedasticity, and how does it affect the model?\n",
        "# Heteroscedasticity means non-constant variance in residuals, leading to unreliable predictions.\n",
        "\n",
        "# 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "# - Remove correlated predictors.\n",
        "# - Use Principal Component Analysis (PCA).\n",
        "# - Apply Ridge or Lasso Regression.\n",
        "\n",
        "# 13. What are common techniques for transforming categorical variables?\n",
        "# - One-hot encoding\n",
        "# - Label encoding\n",
        "# - Dummy variables\n",
        "\n",
        "# 14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "# Interaction terms help capture combined effects of independent variables.\n",
        "\n",
        "# 15. How can the interpretation of intercept differ in Simple vs. Multiple Linear Regression?\n",
        "# - Simple: Represents Y when X = 0.\n",
        "# - Multiple: Represents Y when all independent variables = 0.\n",
        "\n",
        "# 16. What is the significance of the slope in regression analysis?\n",
        "# It determines how much the dependent variable changes per unit increase in an independent variable.\n",
        "\n",
        "# 17. How does the intercept provide context for the relationship between variables?\n",
        "# It provides a baseline value of Y when all independent variables are zero.\n",
        "\n",
        "# 18. What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "# - Does not indicate if the model is overfitting.\n",
        "# - A high R¬≤ does not always mean a good model.\n",
        "# - It does not account for the number of predictors.\n",
        "\n",
        "# 19. How would you interpret a large standard error for a regression coefficient?\n",
        "# It indicates a high level of uncertainty in the coefficient estimate.\n",
        "\n",
        "# 20. How can heteroscedasticity be identified in residual plots?\n",
        "# If residuals show a funnel shape, heteroscedasticity is present.\n",
        "\n",
        "# 21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "# It suggests that additional variables are not contributing meaningful information.\n",
        "\n",
        "# 22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "# To ensure that all features contribute equally and to prevent bias in coefficients.\n",
        "\n",
        "# üìå Polynomial Regression\n",
        "\n",
        "# 23. What is polynomial regression?\n",
        "# A regression model that fits a polynomial equation instead of a straight line.\n",
        "\n",
        "# 24. How does polynomial regression differ from linear regression?\n",
        "# - Linear Regression fits a straight line.\n",
        "# - Polynomial Regression fits a curved trendline.\n",
        "\n",
        "# 25. When is polynomial regression used?\n",
        "# When the relationship between X and Y is non-linear.\n",
        "\n",
        "# 26. What is the general equation for polynomial regression?\n",
        "# Y = b0 + b1X + b2X¬≤ + ... + bnX‚Åø\n",
        "\n",
        "# 27. Can polynomial regression be applied to multiple variables?\n",
        "# Yes, it can be extended to Multiple Polynomial Regression.\n",
        "\n",
        "# 28. What are the limitations of polynomial regression?\n",
        "# - Overfitting when the degree is too high.\n",
        "# - Difficult interpretation of coefficients.\n",
        "# - Computationally expensive.\n",
        "\n",
        "# 29. What methods can be used to evaluate model fit for polynomial regression?\n",
        "# - R¬≤ and Adjusted R¬≤\n",
        "# - Mean Squared Error (MSE)\n",
        "# - Cross-validation\n",
        "\n",
        "# 30. Why is visualization important in polynomial regression?\n",
        "# It helps identify patterns, check model fit, and avoid overfitting.\n",
        "\n",
        "# 31. How is polynomial regression implemented in Python?\n",
        "\n"
      ],
      "metadata": {
        "id": "SluHkD-6Boi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "# Convert to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kUbydj-DCtD",
        "outputId": "dd75198a-bea7-4af1-c938-9588fc81f42a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.  5. 10. 17. 26.]\n"
          ]
        }
      ]
    }
  ]
}