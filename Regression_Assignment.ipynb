{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression\n",
        "\n",
        "# 1. What is Simple Linear Regression?\n",
        "# - A statistical method that models the relationship between a dependent variable (Y)\n",
        "#   and an independent variable (X) using the equation: Y = mX + c.\n",
        "\n",
        "# 2. What are the key assumptions of Simple Linear Regression?\n",
        "# - Linearity: Relationship between X and Y is linear.\n",
        "# - Independence: Observations are independent.\n",
        "# - Homoscedasticity: Constant variance of residuals.\n",
        "# - Normality: Residuals should follow a normal distribution.\n",
        "# - No Multicollinearity: Only one independent variable.\n",
        "\n",
        "# 3. What does the coefficient m represent in Y = mX + c?\n",
        "# - The slope m represents the rate of change in Y for a one-unit change in X.\n",
        "\n",
        "# 4. What does the intercept c represent in Y = mX + c?\n",
        "# - The intercept c is the value of Y when X = 0.\n",
        "\n",
        "# 5. How do we calculate the slope m?\n",
        "# - Formula: m = (Σ(Xi - X̄)(Yi - Ȳ)) / (Σ(Xi - X̄)²)\n",
        "\n",
        "# 6. Purpose of the least squares method:\n",
        "# - It minimizes the sum of squared residuals to find the best-fit line.\n",
        "\n",
        "# 7. Interpretation of R² in Simple Linear Regression:\n",
        "# - R² measures how well the regression line fits the data (closer to 1 = better fit).\n",
        "\n",
        "# Multiple Linear Regression\n",
        "\n",
        "# 8. What is Multiple Linear Regression?\n",
        "# - A model that uses two or more independent variables to predict Y.\n",
        "\n",
        "# 9. Difference between Simple and Multiple Linear Regression:\n",
        "# - Simple: 1 independent variable.\n",
        "# - Multiple: 2 or more independent variables.\n",
        "\n",
        "# 10. Key assumptions of Multiple Linear Regression:\n",
        "# - Linearity, Independence, Homoscedasticity, Normality, No Multicollinearity.\n",
        "\n",
        "# 11. What is heteroscedasticity, and how does it affect the model?\n",
        "# - It refers to non-constant variance in residuals, leading to biased estimates.\n",
        "\n",
        "# 12. How to handle multicollinearity?\n",
        "# - Remove correlated predictors, use PCA, or apply Ridge/Lasso Regression.\n",
        "\n",
        "# 13. Transforming categorical variables for regression:\n",
        "# - One-hot encoding, Label encoding, Dummy variables.\n",
        "\n",
        "# 14. Role of interaction terms:\n",
        "# - Captures the combined effects of independent variables.\n",
        "\n",
        "# 15. Interpretation of intercept in Simple vs. Multiple Linear Regression:\n",
        "# - Simple: Intercept represents Y when X = 0.\n",
        "# - Multiple: Intercept represents Y when all independent variables = 0.\n",
        "\n",
        "# 16. Significance of slope in regression analysis:\n",
        "# - Determines the effect of an independent variable on Y.\n",
        "\n",
        "# 17. How does the intercept provide context in regression?\n",
        "# - Represents the baseline value of Y when all X values are zero.\n",
        "\n",
        "# 18. Limitations of R² as a model performance measure:\n",
        "# - Does not indicate overfitting, does not validate correctness, ignores predictor count.\n",
        "\n",
        "# 19. Interpretation of a large standard error for a regression coefficient:\n",
        "# - Indicates high variability in the coefficient estimate.\n",
        "\n",
        "# 20. How to identify heteroscedasticity in residual plots?\n",
        "# - Residuals forming a funnel shape indicate heteroscedasticity.\n",
        "\n",
        "# 21. Meaning of high R² but low adjusted R² in Multiple Linear Regression:\n",
        "# - Suggests that additional predictors are not contributing meaningful information.\n",
        "\n",
        "# 22. Importance of scaling variables in Multiple Linear Regression:\n",
        "# - Prevents large feature values from dominating and affecting model accuracy.\n",
        "\n",
        "# Polynomial Regression\n",
        "\n",
        "# 23. What is polynomial regression?\n",
        "# - A regression model that fits a polynomial equation instead of a straight line.\n",
        "\n",
        "# 24. Difference between Linear & Polynomial Regression:\n",
        "# - Linear Regression fits a straight line; Polynomial Regression fits a curved trendline.\n",
        "\n",
        "# 25. When is polynomial regression used?\n",
        "# - When the relationship between X and Y is non-linear.\n",
        "\n",
        "# 26. General equation for polynomial regression:\n",
        "# - Y = b0 + b1X + b2X² + ... + bnXⁿ\n",
        "\n",
        "# 27. Can polynomial regression be applied to multiple variables?\n",
        "# - Yes, it can be extended to Multiple Polynomial Regression.\n",
        "\n",
        "# 28. Limitations of polynomial regression:\n",
        "# - Overfitting when the degree is too high, difficult interpretation, computationally expensive.\n",
        "\n",
        "# 29. Methods to evaluate polynomial regression model fit:\n",
        "# - R², Adjusted R², Mean Squared Error (MSE), Cross-validation.\n",
        "\n",
        "# 30. Why is visualization important in polynomial regression?\n",
        "# - Helps detect patterns, check model fit, and avoid overfitting.\n",
        "\n",
        "# 31. Python Implementation of Polynomial Regression\n"
      ],
      "metadata": {
        "id": "SluHkD-6Boi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "# Convert to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kUbydj-DCtD",
        "outputId": "dd75198a-bea7-4af1-c938-9588fc81f42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.  5. 10. 17. 26.]\n"
          ]
        }
      ]
    }
  ]
}